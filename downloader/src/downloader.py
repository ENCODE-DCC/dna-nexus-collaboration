#!/usr/bin/env python
# downloader 0.0.1
# Generated by dx-app-wizard.
#
# Parallelized execution pattern: Your app will generate multiple jobs
# to perform some computation in parallel, followed by a final
# "postprocess" stage that will perform any additional computations as
# necessary.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os
import dxpy
import gzip
import csv
import subprocess
import threading

mirror_urls = [
    "http://encodedcc.sdsc.edu/warehouse/",
    "http://encode-01.sdsc.edu/warehouse/"
]

class Map(dict):
    def __init__(self, **kwargs):
        super(Map, self).__init__(**kwargs)
        self.__dict__ = self

# load the snapshot tsv as [[filepath,size,md5],...]
def load_file_list():
    with gzip.open('/ENCODE-SDSC-snapshot-20140505.tsv.gz') as tsvin:
        tsvin = csv.reader(tsvin, delimiter='\t')
        tsvin.next() # skip header
        return [Map(path=row[0], size=int(row[1]), md5=row[2]) for row in tsvin]

# ensure existence of all necessary folders in the project
def mkdirs():
    files = load_file_list()
    dirs = set([os.path.dirname(f.path) for f in files])

    proj = dxpy.DXProject(dxpy.PROJECT_CONTEXT_ID)
    for dn in dirs:
        proj.new_folder("/" + dn, parents=True)

@dxpy.entry_point("postprocess")
def postprocess(process_outputs):
    # Change the following to process whatever input this stage
    # receives.  You may also want to copy and paste the logic to download
    # and upload files here as well if this stage receives file input
    # and/or makes file output.

    for output in process_outputs:
        pass

    return { "answer": "placeholder value" }


def transfer_file(f):
    dn, fn = os.path.split(f.path)
    dn = "/" + dn
    urls = [base + f.path for base in mirror_urls]

    # download
    subprocess.check_call(["aria2c","-x","16", "-s","64","--check-certificate=false","-o",f.md5] + urls)
    
    # verify integrity
    md5 = subprocess.Popen(["md5sum",f.md5],stdout=subprocess.PIPE).communicate()[0]
    if md5 != f.md5:
        raise Error("expected MD5 " + f.md5 + " got " + md5)
    
    # upload to project
    subprocess.check_call(["/ua","-p",dxpy.PROJECT_CONTEXT_ID,"-f",dn,"-n",fn,"-r","8",
                           "--do-not-compress","--do-not-resume",fn])
    
    # tag file with md5
    dxfiles = dxpy.search.find_data_objects(project=dxpy.PROJECT_CONTEXT_ID,classname="file",
                                            folder=dn,name=fn,return_handler=True)
    if len(dxfiles) != 1:
        raise Error("could not uniquely locate file just uploaded")
    dxfiles[0].set_properties({"md5": f.md5})

    # delete scratch copy
    os.remove(f.md5)

def process_file(f):
    print f.path, "begin"
    dn, fn = os.path.split(f.path)
    dn = "/" + dn

    # check if a file object with this path already exists in the project
    existing_dxfiles = dxpy.search.find_data_objects(project=dxpy.PROJECT_CONTEXT_ID,classname="file",
                                                     folder=dn,name=fn,return_handler=True)

    if len(existing_dxfiles) > 1:
        raise Error(f.path + " found multiple existing file objects! manually remove them and try again")
    elif len(existing_dxfiles) == 1:
        existing_dxfile = existing_dxfiles[0]

        if existing_dxfile.state == "open":
            print f.path, " removing existing open file object"
            existing_dxfile.remove()
        else:
            existing_props = existing_dxfile.get_properties()
            if "md5" not in existing_props or existing_props["md5"] != f.md5:
                raise Error(f.path + " MD5 mismatch with existing file object! manually remove and try again")
            print f.path, " skip"
            return 0

    max_tries=8
    for n in xrange(max_tries)
        try:
            transfer_file(f)
            break
        except:
            print f.path, sys.exc_info()[0]
            if n == max_tries-1:
                raise
            print f.path, " retry"

    print f.path, " complete"
    return 1

@dxpy.entry_point("process")
def process(workers, max_files_per_worker, whoami, smallest):
    # TODO spawn dstat
    subprocess.check_call("gunzip /ua.gz; chmod +x /ua",shell=True)
    files = load_file_list()

    # sort the files by MD5
    if not smallest:
        files.sort(key=lambda f: f.md5)
    else:
        # for testing: start with the smallest files
        files.sort(key=lambda f: f.size)

    # take only those for this worker
    files = [files[i] for i in xrange(len(files)) if i%workers == whoami]

    # apply max_files_per_worker
    if len(files) > max_files_per_worker:
        del files[max_files_per_worker:]

    print "will process", len(files), "files"

    # TODO use multiprocessing.dummy.Pool to do the following
    #      count skipped & transferred

    for f in files:
       process_file(f)

    return { "output": "placeholder value" }

@dxpy.entry_point("main")
def main(workers, max_files_per_worker=None, smallest=Falso):
    mkdirs()

    subjobs = []
    for i in range(workers):
        subjob_input = { "workers": workers, "max_files_per_worker": max_files_per_worker, "whoami": i, "smallest": smallest }
        subjobs.append(dxpy.new_dxjob(subjob_input, "process"))

    # The following line creates the job that will perform the
    # "postprocess" step of your app.  We've given it an input field
    # that is a list of job-based object references created from the
    # "process" jobs we just created.  Assuming those jobs have an
    # output field called "output", these values will be passed to the
    # "postprocess" job.  Because these values are not ready until the
    # "process" jobs finish, the "postprocess" job WILL NOT RUN until
    # all job-based object references have been resolved (i.e. the
    # jobs they reference have finished running).
    #
    # If you do not plan to have the "process" jobs create output that
    # the "postprocess" job will require, then you can explicitly list
    # the dependencies to wait for those jobs to finish by setting the
    # "depends_on" field to the list of subjobs to wait for (it
    # accepts either dxpy handlers or string IDs in the list).  We've
    # included this parameter in the line below as well for
    # completeness, though it is unnecessary if you are providing
    # job-based object references in the input that refer to the same
    # set of jobs.

    postprocess_job = dxpy.new_dxjob(fn_input={ "process_outputs": [subjob.get_output_ref("output") for subjob in subjobs] },
                                     fn_name="postprocess",
                                     depends_on=subjobs)

    # If you would like to include any of the output fields from the
    # postprocess_job as the output of your app, you should return it
    # here using a job-based object reference.  If the output field in
    # the postprocess function is called "answer", you can pass that
    # on here as follows:
    #
    # return { "app_output_field": postprocess_job.get_output_ref("answer"), ...}
    #
    # Tip: you can include in your output at this point any open
    # objects (such as gtables) which will be closed by a job that
    # finishes later.  The system will check to make sure that the
    # output object is closed and will attempt to clone it out as
    # output into the parent container only after all subjobs have
    # finished.

    output = {}

    return output

dxpy.run()
